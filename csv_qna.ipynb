{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e49aa561-0c4a-4415-ba75-7bd45862bc9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "from langchain.agents import create_csv_agent\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40d45f50-8cf1-4d22-bb4e-c29da4e24f19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders.csv_loader import CSVLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cb30902-5426-45cd-91bf-49d8083cd4ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import csv\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "loader = CSVLoader(file_path='data/YouTube_transcripts_Kaggle.csv', \n",
    "                  csv_args={\n",
    "                      \"delimiter\":\",\",\n",
    "                      # \"fieldnames\":[\"title\", \"author\", \"transcript\", \"playlist_name\"],\n",
    "                      },\n",
    "                   # source_column=\"transcript\"\n",
    "                  )\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c1d00e6-5d9e-4b37-8404-eff504c7404f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"title: Hyperparameter Tuning on Vertex AI\\nauthor: Google Cloud Tech\\ntranscript: PRIYANKA VERGADIA: Hi. I'm Priyanka Vergadia. And in this video, we will\\nsee what are hyperparameters, how to tune them, and set\\nup a hyperparameter tuning job in Vertex AI. All right, so what\\nis a hyperparameter? Well, a training application\\nhandles three categories of data as it trains the model. The input data, which is\\nalso called training data, contains the features that the\\nmodel uses to make predictions. Now, but this data\\nis never directly part of the model\\narchitecture itself. The second type of data\\nis model parameters. These are the variables that\\nour ML model or technique uses to adjust the data. For example, a deep\\nneural network, or DNN, is composed of processing\\nnodes or neurons, each with an operation\\nperformed on the data as it travels\\nthrough the network. Now, when our DNN is\\ntrained, each node has a weight value that tells\\nour model how much impact it has on the final prediction. Now, they are what distinguishes\\nour particular model from other models of the\\nsame architecture trained on different data. Now, the third type of\\ndata is hyperparameters. And these are the\\nvariables that govern the training process itself. For example, a part of\\ndesigning a deep neural network is deciding how\\nmany hidden layers you will want between the\\ninput and the output to be, and how many nodes each\\nhidden layer would use. Like here I have two,\\nthen three, then three, but this could be\\nany number of nodes. These variables are not directly\\nrelated to the training data, and they are the configuration\\nvariables for your model. Now, when it comes\\nto model parameters, they can change with\\nthe training job. But hyperparameters\\nare usually going to stay constant, which means,\\nin our deep neural network model example, finding the\\nright number of hidden layers means choosing a value of\\nthe number of hidden layers, training a model, evaluating\\nit with those values, and then trying it again\\nwith a different value. Now, if we wanted to get the\\nbest type of parameter settings manually, it could get pretty\\ntedious pretty quickly. That's where Vertex AI\\nhyperparameter tuning comes in. It is an automated process\\nof tuning hyperparameter jobs as you're running your\\nmultiple trials of the training application with values of\\nthe chosen hyperparameters set within the limits\\nthat you specify. And to select hyperparameters\\nthat give the best performance, Vertex AI uses the\\nBayesian optimization for hyperparameter tuning. If you're interested\\nin the math behind it, I've included a link below\\nto learn more about it. With that, let's\\njump into the console and train a\\nhyperparameter tuning job. Here I am in the\\nGoogle Cloud console. First, you need to enable\\nCompute Engine and Container Registry APIs. In this demo, I'm training and\\ntuning an image classification model trained on the\\nhorses or humans data set from TensorFlow data sets. We will need a notebook to\\ncontainerize our training application code. Since I'm creating a TensorFlow\\nmodel in this example, I'm going to select a\\nTensorFlow Enterprise 2.5 notebook without a GPU. But you can train a model\\nbuilt with any framework using custom or pre-built\\ncontainers, so select the notebook accordingly. I already created one here,\\nso let's just hop into that. In the terminal, I created a\\nfolder called Horses or Humans, and inside that folder,\\nI've created a Docker file. This Docker file uses deep\\nlearning container TensorFlow Enterprise 2.5 GPU Docker image. And the deep learning\\ncontainers on Google Cloud actually come with many common\\nmachine learning and data science frameworks\\nthat are pre-installed. We're also installing the\\nHypertune Library here, which we will use later\\nto report the metrics we want to optimize on Vertex AI. After downloading the\\nimage, the Docker file sets up the entry point\\nfor the training code. Then, I created the Trainer\\nfolder with task.py file and added the code for\\ntraining and tuning the model. Let's take a deeper\\nlook at this code now. There are a few components\\nthat are specific to using hyperparameter tuning service. The script imports\\nthe Hypertune Library. We installed this library in the\\nDocker file we created earlier. And the function, get_args,\\ndefines a command line argument for each hyperparameter\\nthat you want to tune. In this example,\\nthe hyperparameters that will be tuned\\nare the learning rate, the momentum value\\nin the optimizer, and the number of neurons in the\\nlast hidden layer of the model. But feel free to experiment\\nwith others, as well. The value passed\\nin these arguments is then used to set the\\ncorresponding parameters in the code. At the end of main function,\\nthe Hypertune Library is used to define the\\nmetric you want to optimize. In TensorFlow, declares\\nmodel.fit method returns a history object. The history.history\\nattribute is a record of training loss\\nvalues and metrics values at successive epochs. If you pass validation\\ndata to model.fit, the history.history attribute\\nwill include validation loss and metrics values as well. If you want the hyperparameter\\ntuning service to discover the values that maximize the\\nmodel's validation accuracy, you define the metric\\nas the last entry, or NUM_EPOCHS as minus 1\\nof the val_accuracy list. Then pass this metric to\\nan instance of hypertune. You can pick whatever\\nstring you like for the hyperparameter\\nmetric tag. I'm setting it to accuracy here. But you will need to\\nuse the same string again later when you kick off\\nthe hyperparameter tuning job. Now we're ready to\\nbuild the container. Set the project ID, image\\nURI, build the container using the image URI. Once the container\\nis built, push it to the container registry. And then next, let's\\nhead into the Vertex AI console and create a model to\\nrun our hyperparameter tuning job. To configure a training\\njob, select No Manage Data Set, Custom Training,\\ngive the model a name, and then select the custom\\ncontainer that we just created from our notebook. Notice that here, I'm\\nusing Custom Training via Custom Container on Google\\nCloud Container Registry. But you can also run a\\nhyperparameter tuning job with the pre-built containers. In the next step, enable\\nhyperparameter tuning, add learning rate, which would\\nbe double, min and max values, and log scaling. Another parameter,\\nwhich is momentum, type would be double,\\nminimum and maximum values, linear scaling. And third\\nhyperparameter would be on number of neurons,\\ndiscrete type, and of value, and no scaling for this one. Next, we need to provide the\\nmetric we want to optimize as well as the goal. This should be the same\\nas our hyperparameter metric tag we set in the\\ntraining application. It was set to accuracy, and\\nour goal is to maximize it. The Vertex AI hyperparameter\\ntuning service will run multiple trials\\nof our training application with these values. You will need to put an upper\\nbound on the number of trials this service will run. For demo purposes,\\nI'm adding 15 here. More trials generally\\nlead to better results, but there will be a point\\nof diminishing returns after which\\nadditional trials have little or no effect\\non the metric you're trying to optimize. So it's always a best practice\\nto start with a smaller number of trials and get\\na sense of how impactful your chosen hyperparameters\\nare before scaling up to a larger number of trials. Now you'll also need to set\\nan upper bound on the number of parallel trials. Here, I'm setting this to three. Increasing the number\\nof parallel trials will reduce the amount of\\ntime the hyperparameter tuning job takes to actually run. However, it can reduce the\\neffectiveness of the job overall. This is because the\\ndefault tuning strategy uses results of previous\\ntrials to inform the assignment of values\\nin the subsequent trials. So if you run too many\\ntrials in parallel, there will be trials that\\nstart without the benefit of the results from the\\ntrial that are still running. Now the last step is to\\nselect the search algorithm. Now grid search is a very\\ntraditional technique for implementing\\nhyperparameters. It's a brute-force, combines\\nall the combinations. Now random search randomly\\nsamples the search space and evaluates the sets\\nfor specified probability distribution. For example, instead of trying\\nto check all the 100,000 samples, we can check 1,000\\nrandom sample parameters. We will select default\\nsearch algorithm here, which will use Google Vizier to\\nperform Bayesian optimization for hyperparameter tuning. To learn more about\\nhow this works, see the documentation below. In Compute and Pricing, leave\\nthe selected region as is and configure Worker Pool\\nzero with a machine type, accelerator, and disk size. Now note here that selecting\\nGPU is totally optional-- hyperparameter\\ntuning job will just take a little longer to complete\\nif you do not use a GPU. Then start the training. It should take about\\n45 minutes to train with the demo parameters\\nthat I've selected here. And when it's\\nfinished, you will be able to click on the job\\nname and see the results of the tuning trials. From the results\\nof our trial, we can see here that setting\\nthe learning rate to 3.9, momentum to 2.8, and neurons\\nto 128 results in the highest validation accuracy. In this video,\\nwe've learned what is hyperparameter tuning,\\nhow Vertex AI helps automate the hyperparameter\\ntuning process, and, then, we launched a\\nhyperparameter tuning job for training code in Custom\\nContainer via the UI. If you would like to do the\\nsame using Vertex Python SDK, I have linked the codelab below. Give it a try, let me know how\\nit goes, and for any questions, reach out to me on\\nTwitter @pvergadia.\\nplaylist_name: AI and Machine Learning with Google Cloud\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[2].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e92d6f6-cf8b-452f-b607-cdc437d68fa8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    CharacterTextSplitter,\n",
    ")\n",
    "from langchain.vectorstores import Chroma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "671f6706-57a1-485f-a6a5-ad1843cb7a9b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "pages = text_splitter.split_text(data[0].page_content)\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "texts = text_splitter.create_documents(pages)\n",
    "\n",
    "# print(texts)\n",
    "len(texts)\n",
    "\n",
    "# dataset_path = \"hub://\" + org + \"/data\"\n",
    "# embeddings = OpenAIEmbeddings()\n",
    "# db = DeepLake.from_documents(\n",
    "#     texts, embeddings, dataset_path=dataset_path, overwrite=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1294c5a0-6a2c-4bd0-9bd4-c343033d2b9f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[1].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "92cc9ee1-e789-4749-93a4-81c4b5445cf3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n"
     ]
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY, model=\"text-embedding-ada-002\")\n",
    "vectorstore = Chroma.from_documents(texts,embedding=embeddings, persist_directory=\"./chroma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0cb53d0-d3d8-46ce-be1e-c5d559a02e5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import\n",
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "# load the document and split it into chunks\n",
    "\n",
    "# create the open-source embedding function\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# load it into Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b64fed3-8f4b-4155-b4e4-3e65a70b1db9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "db = Chroma.from_documents(texts, embedding_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0f48a9d7-7ca8-46a6-8aa3-02b3d3e348ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# query it\n",
    "query = \"how to use feature store\"\n",
    "docs = vectorstore.similarity_search_with_score(query)\n",
    "\n",
    "# print results\n",
    "# print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "54d35e49-6a7a-46eb-bc96-886b01f3183a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content=\"quality to degrade over time. That is exactly where Vertex\\nFeature Store comes in. It's a fully managed\\nand unified solution to share, discover,\\nand serve machine learning features at scale\\nacross different teams within your organization. And it also helps\\nreduce the time to build and deploy\\nyour AI ML applications by making it easy to\\nmanage and organize your ML features in one place. It makes the features reusable,\\neasy to serve, and avoids skew. Now let's see how to set it up. In the console, in Vertex\\nAI, we see the Feature tab. To get started, let's\\nclick on this documentation and explore, using\\nFeature Store section. Now the first thing you\\nneed is a Feature Store. At the time of this\\nrecording, Feature Store is in preview so just\\nknow that depending on when you're watching this,\\nthere might be more options and updates that you would see. You cannot create a Feature\\nStore in the console, so let's use this sample\", metadata={}),\n",
       "  0.2988402545452118),\n",
       " (Document(page_content=\"three hierarchical concepts. Featurestore, which is the\\nplace to store your features-- EntityType, under Featurestore,\\ndescribes an object to be modeled, real or\\nvirtual, and Feature itself, under EntityType, describes an\\nattribute of that EntityType. Now in our movie's\\nprediction example, we will create a featurestore\\ncalled movie_prediction. This store has\\ntwo entity types-- Users and Movies. The user's entity\\ntype has age, gender, and like genres features. The Movies entity type has\\nthe genre and average rating features. The first thing we do is\\nto create the featurestore. The method to create\\na featurestore returns a long\\nrunning operation that starts an asynchronous job. This may take about\\nthree minutes or so, and once the\\nfeaturestore is created, we can see it in the console. And we can create our\\nentities in this store. I'm creating two entities here-- user and movies. We can also create features\\nwithin these entities. Here I've created age,\", metadata={}),\n",
       "  0.3193686902523041),\n",
       " (Document(page_content=\"Store in the console, so let's use this sample\\nnotebook to learn how to create it using the SDK. This sample uses a movie\\nrecommendations data set and the task\\nis to train a model to predict if a user is\\ngoing to watch a movie and serve this model online. We will learn to import our\\nfeatures into Feature Store, serve online prediction requests\\nusing the imported features, and then access imported\\nfeatures in offline jobs, such as training jobs. To set up, we install\\nsome additional packages, set up our project, and\\nauthenticate our Google Cloud account. Step one is to create\\ndata set for output. We are creating BigQuery data\\nset to host the output data. Input the name of the\\ndata set and the table we want to store\\nthe output later. Then we are defining\\nconstants and Feature Store-related imports. Here's how the Vertex\\nFeature Store actually works. It organizes the data with the\\nthree hierarchical concepts. Featurestore, which is the\", metadata={}),\n",
       "  0.3222348392009735),\n",
       " (Document(page_content=\"entity per request or even read multiple\\nentities per request. Now if you need feature\\nvalues for high throughput, typically for training a\\nmodel or batch prediction, then serving feature\\nvalues in batch is a better idea\\nthan serving online. Consider this example-- if the\\ntask is to prepare a training data set to train a model which\\npredicts if a given user will watch a given movie, then to\\nachieve this we need two sets of inputs-- features, that we\\nhave already imported, and labels, which is the\\nground-truth data recorder that user X has watched\\nmovie Y. It also includes the time stamp which\\nindicates when the ground-truth was actually observed. As labels and feature values\\nare collected over time, those feature values change. The Feature Store can perform\\na point in time lookup so that you can\\nfetch the feature values at a particular time. It's literally the data\\nversion of going back to a previous version of\\nyour source code in GitHub. Imagine freezing the\", metadata={}),\n",
       "  0.37957319617271423)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aacb4bda-6b55-4761-89e4-da505ed6abd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "\n",
    "# db = Chroma.from_documents(texts, embeddings)\n",
    "# expose this index in a retriever interface\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":2})\n",
    "\n",
    "# create a chain to answer questions \n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=OpenAI(openai_api_key=OPENAI_API_KEY), chain_type=\"stuff\", retriever=retriever, return_source_documents=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a1dd78d3-be00-4f07-9a1a-5ff257e0c509",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"How to use the feature store?\"\n",
    "result = qa({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9ce5fdae-e839-49a4-83de-8113eeecea97",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'How to use the feature store?',\n",
       " 'result': ' To use the feature store, you need to create a Feature Store using the SDK. You can follow the instructions in the sample notebook which shows how to import features into the Feature Store, serve online prediction requests using the imported features, and access imported features in offline jobs such as training jobs. The Feature Store organizes data with three hierarchical concepts: Feature Store, Feature Group, and Feature.',\n",
       " 'source_documents': [Document(page_content=\"quality to degrade over time. That is exactly where Vertex\\nFeature Store comes in. It's a fully managed\\nand unified solution to share, discover,\\nand serve machine learning features at scale\\nacross different teams within your organization. And it also helps\\nreduce the time to build and deploy\\nyour AI ML applications by making it easy to\\nmanage and organize your ML features in one place. It makes the features reusable,\\neasy to serve, and avoids skew. Now let's see how to set it up. In the console, in Vertex\\nAI, we see the Feature tab. To get started, let's\\nclick on this documentation and explore, using\\nFeature Store section. Now the first thing you\\nneed is a Feature Store. At the time of this\\nrecording, Feature Store is in preview so just\\nknow that depending on when you're watching this,\\nthere might be more options and updates that you would see. You cannot create a Feature\\nStore in the console, so let's use this sample\", metadata={}),\n",
       "  Document(page_content=\"Store in the console, so let's use this sample\\nnotebook to learn how to create it using the SDK. This sample uses a movie\\nrecommendations data set and the task\\nis to train a model to predict if a user is\\ngoing to watch a movie and serve this model online. We will learn to import our\\nfeatures into Feature Store, serve online prediction requests\\nusing the imported features, and then access imported\\nfeatures in offline jobs, such as training jobs. To set up, we install\\nsome additional packages, set up our project, and\\nauthenticate our Google Cloud account. Step one is to create\\ndata set for output. We are creating BigQuery data\\nset to host the output data. Input the name of the\\ndata set and the table we want to store\\nthe output later. Then we are defining\\nconstants and Feature Store-related imports. Here's how the Vertex\\nFeature Store actually works. It organizes the data with the\\nthree hierarchical concepts. Featurestore, which is the\", metadata={})]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b100cb1b-0544-4138-884b-6f1e75a6cd6b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "query = \"How to use neural networks to train Alexnet?\"\n",
    "result_02 = qa({\"query\": query})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "47cf467e-8275-4129-9d05-6d4c5ac29bc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_res(res):\n",
    "    print(res['result'])\n",
    "    print(\"Docs::\")\n",
    "    for i, doc in enumerate(res[\"source_documents\"]):\n",
    "        print(f'{i}')\n",
    "        print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7e51cc66-a61a-49f7-b95c-be2c022473de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I don't know.\n",
      "Docs::\n",
      "0\n",
      "Store in the console, so let's use this sample\n",
      "notebook to learn how to create it using the SDK. This sample uses a movie\n",
      "recommendations data set and the task\n",
      "is to train a model to predict if a user is\n",
      "going to watch a movie and serve this model online. We will learn to import our\n",
      "features into Feature Store, serve online prediction requests\n",
      "using the imported features, and then access imported\n",
      "features in offline jobs, such as training jobs. To set up, we install\n",
      "some additional packages, set up our project, and\n",
      "authenticate our Google Cloud account. Step one is to create\n",
      "data set for output. We are creating BigQuery data\n",
      "set to host the output data. Input the name of the\n",
      "data set and the table we want to store\n",
      "the output later. Then we are defining\n",
      "constants and Feature Store-related imports. Here's how the Vertex\n",
      "Feature Store actually works. It organizes the data with the\n",
      "three hierarchical concepts. Featurestore, which is the\n",
      "1\n",
      "title: Introduction to Vertex AI Feature Store\n",
      "author: Google Cloud Tech\n",
      "transcript: PRIYANKA VERGADIA:\n",
      "Did you know that most of the time spent\n",
      "by data scientists goes into wrangling data? More specifically, in\n",
      "feature engineering, which is transforming raw\n",
      "data into high-quality input signals for ML models. But this process is often\n",
      "inefficient and brittle. Well I'm Priyanka,\n",
      "and in this video we will identify\n",
      "the key challenges with feature engineering, how\n",
      "Vertex Feature Store help solve them, and see a quick demo. Now what are the key\n",
      "challenges with ML features? The first is that they are\n",
      "hard to share and reuse across your different\n",
      "steps of the ML workflow and across projects, which\n",
      "results in duplicate efforts. Second is that it is hard to\n",
      "serve in production reliably with lower latency,\n",
      "and the third is that there is an\n",
      "inadvertent skew in feature values between\n",
      "training and serving, usually, which causes your model\n",
      "quality to degrade over time. That is exactly where Vertex\n"
     ]
    }
   ],
   "source": [
    "print_res(result_02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "41d4bdba-ffc9-4d46-aac4-e4921ade55bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'How to use neural networks to train Alexnet?',\n",
       " 'result': \" I don't know.\",\n",
       " 'source_documents': [Document(page_content=\"Store in the console, so let's use this sample\\nnotebook to learn how to create it using the SDK. This sample uses a movie\\nrecommendations data set and the task\\nis to train a model to predict if a user is\\ngoing to watch a movie and serve this model online. We will learn to import our\\nfeatures into Feature Store, serve online prediction requests\\nusing the imported features, and then access imported\\nfeatures in offline jobs, such as training jobs. To set up, we install\\nsome additional packages, set up our project, and\\nauthenticate our Google Cloud account. Step one is to create\\ndata set for output. We are creating BigQuery data\\nset to host the output data. Input the name of the\\ndata set and the table we want to store\\nthe output later. Then we are defining\\nconstants and Feature Store-related imports. Here's how the Vertex\\nFeature Store actually works. It organizes the data with the\\nthree hierarchical concepts. Featurestore, which is the\", metadata={}),\n",
       "  Document(page_content=\"title: Introduction to Vertex AI Feature Store\\nauthor: Google Cloud Tech\\ntranscript: PRIYANKA VERGADIA:\\nDid you know that most of the time spent\\nby data scientists goes into wrangling data? More specifically, in\\nfeature engineering, which is transforming raw\\ndata into high-quality input signals for ML models. But this process is often\\ninefficient and brittle. Well I'm Priyanka,\\nand in this video we will identify\\nthe key challenges with feature engineering, how\\nVertex Feature Store help solve them, and see a quick demo. Now what are the key\\nchallenges with ML features? The first is that they are\\nhard to share and reuse across your different\\nsteps of the ML workflow and across projects, which\\nresults in duplicate efforts. Second is that it is hard to\\nserve in production reliably with lower latency,\\nand the third is that there is an\\ninadvertent skew in feature values between\\ntraining and serving, usually, which causes your model\\nquality to degrade over time. That is exactly where Vertex\", metadata={})]}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c69a82f-dc21-4a4b-bdb4-68b36b92993b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "llama"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
